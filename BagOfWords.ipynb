{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BagOfWords.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KMR-86/ml-test-processing-basic/blob/master/BagOfWords.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvkH9N8ACOdN",
        "colab_type": "code",
        "outputId": "c5cf56a3-0779-4179-e942-1e05181630a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        }
      },
      "source": [
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "s=[\"hello would you like a cup of coffee with me\", \"bangladesh is the 2nd largest producer of coffee\",\"do you want to have some coffee with me\"]\n",
        "vectorizer = CountVectorizer()\n",
        "x = vectorizer.fit_transform(s)\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 6)\t1\n",
            "  (0, 18)\t1\n",
            "  (0, 19)\t1\n",
            "  (0, 9)\t1\n",
            "  (0, 3)\t1\n",
            "  (0, 11)\t1\n",
            "  (0, 2)\t1\n",
            "  (0, 17)\t1\n",
            "  (0, 10)\t1\n",
            "  (1, 11)\t1\n",
            "  (1, 2)\t1\n",
            "  (1, 1)\t1\n",
            "  (1, 7)\t1\n",
            "  (1, 14)\t1\n",
            "  (1, 0)\t1\n",
            "  (1, 8)\t1\n",
            "  (1, 12)\t1\n",
            "  (2, 19)\t1\n",
            "  (2, 2)\t1\n",
            "  (2, 17)\t1\n",
            "  (2, 10)\t1\n",
            "  (2, 4)\t1\n",
            "  (2, 16)\t1\n",
            "  (2, 15)\t1\n",
            "  (2, 5)\t1\n",
            "  (2, 13)\t1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrpp06KcFrMu",
        "colab_type": "text"
      },
      "source": [
        "Don't understand what is this representation? lets look at the next code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xz-gqUkwEECF",
        "colab_type": "code",
        "outputId": "8cc41ebf-252b-423f-9b66-1a1bf936feb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "print(vectorizer.get_feature_names())\n",
        "print(x.toarray())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['2nd', 'bangladesh', 'coffee', 'cup', 'do', 'have', 'hello', 'is', 'largest', 'like', 'me', 'of', 'producer', 'some', 'the', 'to', 'want', 'with', 'would', 'you']\n",
            "[[0 0 1 1 0 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1]\n",
            " [1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0]\n",
            " [0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXsbW6wfE3gE",
        "colab_type": "text"
      },
      "source": [
        "So, You can see that the features are serialized by sorting. And the actual representation is basically adjacent list version of the sparse matrix to safe some space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkuBjC0PFWBX",
        "colab_type": "text"
      },
      "source": [
        "Now lets see an example of **N-grams**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6sbX9z5FlzA",
        "colab_type": "code",
        "outputId": "45bd6664-17c5-4015-fec1-3024c3c3a8e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "vectorizer_ngram_12 = CountVectorizer(ngram_range=(1, 2))\n",
        "x12 = vectorizer_ngram_12.fit_transform(s)\n",
        "print(x12)\n",
        "print(vectorizer_ngram_12.get_feature_names())\n",
        "print(x12.toarray())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 12)\t1\n",
            "  (0, 35)\t1\n",
            "  (0, 37)\t1\n",
            "  (0, 18)\t1\n",
            "  (0, 6)\t1\n",
            "  (0, 21)\t1\n",
            "  (0, 4)\t1\n",
            "  (0, 33)\t1\n",
            "  (0, 20)\t1\n",
            "  (0, 13)\t1\n",
            "  (0, 36)\t1\n",
            "  (0, 38)\t1\n",
            "  (0, 19)\t1\n",
            "  (0, 7)\t1\n",
            "  (0, 22)\t1\n",
            "  (0, 5)\t1\n",
            "  (0, 34)\t1\n",
            "  (1, 21)\t1\n",
            "  (1, 4)\t1\n",
            "  (1, 22)\t1\n",
            "  (1, 2)\t1\n",
            "  (1, 14)\t1\n",
            "  (1, 27)\t1\n",
            "  (1, 0)\t1\n",
            "  (1, 16)\t1\n",
            "  (1, 23)\t1\n",
            "  (1, 3)\t1\n",
            "  (1, 15)\t1\n",
            "  (1, 28)\t1\n",
            "  (1, 1)\t1\n",
            "  (1, 17)\t1\n",
            "  (1, 24)\t1\n",
            "  (2, 37)\t1\n",
            "  (2, 4)\t1\n",
            "  (2, 33)\t1\n",
            "  (2, 20)\t1\n",
            "  (2, 5)\t1\n",
            "  (2, 34)\t1\n",
            "  (2, 8)\t1\n",
            "  (2, 31)\t1\n",
            "  (2, 29)\t1\n",
            "  (2, 10)\t1\n",
            "  (2, 25)\t1\n",
            "  (2, 9)\t1\n",
            "  (2, 39)\t1\n",
            "  (2, 32)\t1\n",
            "  (2, 30)\t1\n",
            "  (2, 11)\t1\n",
            "  (2, 26)\t1\n",
            "['2nd', '2nd largest', 'bangladesh', 'bangladesh is', 'coffee', 'coffee with', 'cup', 'cup of', 'do', 'do you', 'have', 'have some', 'hello', 'hello would', 'is', 'is the', 'largest', 'largest producer', 'like', 'like cup', 'me', 'of', 'of coffee', 'producer', 'producer of', 'some', 'some coffee', 'the', 'the 2nd', 'to', 'to have', 'want', 'want to', 'with', 'with me', 'would', 'would you', 'you', 'you like', 'you want']\n",
            "[[0 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1\n",
            "  1 1 1 0]\n",
            " [1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0\n",
            "  0 0 0 0]\n",
            " [0 0 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 1 1 1 1 1 0\n",
            "  0 1 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vqz2TiTqGrWu",
        "colab_type": "text"
      },
      "source": [
        "let's play around with the range for a bit:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVwMatQpGv1A",
        "colab_type": "code",
        "outputId": "4d719a99-e0bd-4f0d-e2ea-4e098d794e6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        }
      },
      "source": [
        "vectorizer_ngram_22 = CountVectorizer(ngram_range=(2, 2))\n",
        "x22 = vectorizer_ngram_22.fit_transform(s)\n",
        "print(x22)\n",
        "print(vectorizer_ngram_22.get_feature_names())\n",
        "print(x22.toarray())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 6)\t1\n",
            "  (0, 17)\t1\n",
            "  (0, 18)\t1\n",
            "  (0, 9)\t1\n",
            "  (0, 3)\t1\n",
            "  (0, 10)\t1\n",
            "  (0, 2)\t1\n",
            "  (0, 16)\t1\n",
            "  (1, 10)\t1\n",
            "  (1, 1)\t1\n",
            "  (1, 7)\t1\n",
            "  (1, 13)\t1\n",
            "  (1, 0)\t1\n",
            "  (1, 8)\t1\n",
            "  (1, 11)\t1\n",
            "  (2, 2)\t1\n",
            "  (2, 16)\t1\n",
            "  (2, 4)\t1\n",
            "  (2, 19)\t1\n",
            "  (2, 15)\t1\n",
            "  (2, 14)\t1\n",
            "  (2, 5)\t1\n",
            "  (2, 12)\t1\n",
            "['2nd largest', 'bangladesh is', 'coffee with', 'cup of', 'do you', 'have some', 'hello would', 'is the', 'largest producer', 'like cup', 'of coffee', 'producer of', 'some coffee', 'the 2nd', 'to have', 'want to', 'with me', 'would you', 'you like', 'you want']\n",
            "[[0 0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 1 0]\n",
            " [1 1 0 0 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0 0]\n",
            " [0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 1 1 0 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud4647mqYx_q",
        "colab_type": "text"
      },
      "source": [
        "now to get the distance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7spOX67TY0Nx",
        "colab_type": "code",
        "outputId": "4cb72277-b590-449f-af54-8083712af81f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "#first understand todensde()\n",
        "print(x.todense())\n",
        "#it works just like toarray\n",
        "#but the difference is type\n",
        "#toarray returns an ndarray; todense returns a matrix. If you want a matrix, use todense; otherwise, use toarray.\n",
        "#they also have the same shape! but toarray wont work in this code because euclidean_distances wants a row of a matrix\n",
        "#for ndarray there are other functions available\n",
        "\n",
        "\n",
        "v11=x.todense()\n",
        "v11_=x.toarray()\n",
        "print(v11.shape,\" \",v11_.shape)\n",
        "v12=x12.todense()\n",
        "v22=x22.todense()\n",
        "\n",
        "#for v11\n",
        "#lets try to find out which sentence is more closer to the first sentence\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "for v in v11:\n",
        "  print(euclidean_distances(v11[0],v))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 1 1 0 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1]\n",
            " [1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0]\n",
            " [0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1]]\n",
            "(3, 20)   (3, 20)\n",
            "[[0.]]\n",
            "[[3.60555128]]\n",
            "[[3.16227766]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tP66b56cajO",
        "colab_type": "text"
      },
      "source": [
        "seems like 1 and 3 are closer. which is the correct answer.\n",
        "Now lets try this for v12 and v22:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQ1EfFYTchCy",
        "colab_type": "code",
        "outputId": "96a427c6-a3fa-4c5a-bd3e-ba4f4415245a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "for v in v12:\n",
        "  print(euclidean_distances(v12[0],v))\n",
        "print(\"\\n\\n\")\n",
        "for v in v22:\n",
        "  print(euclidean_distances(v22[0],v))  \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.]]\n",
            "[[5.09901951]]\n",
            "[[4.69041576]]\n",
            "\n",
            "\n",
            "\n",
            "[[0.]]\n",
            "[[3.60555128]]\n",
            "[[3.46410162]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ud5j0XqdGLe",
        "colab_type": "text"
      },
      "source": [
        "Both of them give correct answer as well but ngram(1,2) gives a clear cut better distance. So it is performing better. But it also takes more memory."
      ]
    }
  ]
}